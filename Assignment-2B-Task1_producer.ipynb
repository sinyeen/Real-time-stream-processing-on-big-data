{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asignment 2B: Using real-time streaming data to predict pedestrian traffic\n",
    "\n",
    "**Student Name: Sin Yee Neo**\n",
    "\n",
    "**Student ID: 31340458**\n",
    "\n",
    "Date: 10/02/2021\n",
    "\n",
    "Environment: Python 3.6.0\n",
    "\n",
    "## 1 Producing the data\n",
    "\n",
    "Implement Apache Kafka producer to simulate the real-time data transfer from one repository to another. The program will send 1 batch of all sensor's one day worth of records every 5 seconds to the Kafka stream.\n",
    "\n",
    "### 1.1 Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka import KafkaProducer\n",
    "import random\n",
    "import datetime as dt\n",
    "import csv\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Kafka stream producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function to get list of dict\n",
    "def get_list_of_dict(keys, list_of_tuples):\n",
    "     \"\"\"\n",
    "     This function will accept keys and list_of_tuples as args and return list of dicts\n",
    "     \"\"\"\n",
    "     list_of_dict = [dict(zip(keys, values)) for values in list_of_tuples]\n",
    "     return list_of_dict\n",
    "\n",
    "# Function to read csv file into dictionary\n",
    "def read_csv(fileName):\n",
    "    'Read the CSV file Streaming_Pedestrian_December_counts_per_hour.csv'\n",
    "    data = []\n",
    "    included_cols = ['ID', 'Date_Time', 'Year', 'Month', 'Mdate', 'Day', 'Time', 'Sensor_ID', 'Sensor_Name', 'Hourly_Counts']\n",
    "\n",
    "    with open(fileName) as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            content = list(row[i] for i in included_cols)\n",
    "            data.append(content)\n",
    "    keys = tuple(included_cols)\n",
    "    dict_lst = get_list_of_dict(keys, data)\n",
    "    return dict_lst\n",
    "\n",
    "# Function to publish the message from the csv file\n",
    "def publish_message(producer_instance, topic_name, data):\n",
    "    try:\n",
    "        producer_instance.send(topic_name, data)\n",
    "        print('Message published successfully. Data: ' + str(data))\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message.')\n",
    "        print(str(ex))\n",
    "        \n",
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=['localhost:9092'],\n",
    "                                  value_serializer=lambda x: dumps(x).encode('ascii'),\n",
    "                                  api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka.')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "   \n",
    "    topic = 'pedestrain_count'\n",
    "    cRows = read_csv('Streaming_Pedestrian_December_counts_per_hour.csv')\n",
    "    \n",
    "    print('Publishing record/s..')\n",
    "    producer = connect_kafka_producer()\n",
    "\n",
    "    # create list of dictionaries with keys date, time1 and am/pm\n",
    "    date_dct_lst = []\n",
    "    L = ['date', 'time1', 'am/pm']\n",
    "    for i in range(len(cRows)):\n",
    "        res = {key: cRows[i][key] for key in cRows[0].keys() & {'Date_Time'}} # spling datatime into date and time\n",
    "        split = next(iter(res.values())).split()\n",
    "        res1 = dict(zip(L, split))\n",
    "        date_dct_lst.append(res1)\n",
    "        \n",
    "    # add L into crows\n",
    "    for i in range(len(cRows)):\n",
    "        cRows[i].update(date_dct_lst[i])\n",
    "    \n",
    "    # create a list with unique dates\n",
    "    uniq_date = list(set([d['date'] for d in cRows]))\n",
    "    uniq_date.sort(key=lambda date: datetime.strptime(date, \"%m/%d/%Y\"))\n",
    "    new_date = []\n",
    "    \n",
    "    # create list of lists with data with different unique dates\n",
    "    for dat in uniq_date:\n",
    "        filter_date = [dic for dic in cRows if dic['date'] == dat]\n",
    "        for sub in filter_date:\n",
    "            # without changing datatime to datatime format\n",
    "            sub['ID'] = int(sub['ID'])\n",
    "            sub['Year'] = int(sub['Year'])\n",
    "            sub['Mdate'] = int(sub['Mdate'])\n",
    "            sub['Time'] = int(sub['Time'])\n",
    "            sub['Sensor_ID'] = int(sub['Sensor_ID'])\n",
    "            sub['Hourly_Counts'] = int(sub['Hourly_Counts'])\n",
    "        new_date.append(filter_date)\n",
    "        \n",
    "    # sending data day by day after 5 seconds\n",
    "    starting = 0\n",
    "    while True:\n",
    "        to_send = new_date[starting]\n",
    "        for j in range(len(to_send)):\n",
    "            msg = to_send[j]\n",
    "\n",
    "            publish_message(producer, topic, msg)\n",
    "            \n",
    "        starting += 1\n",
    "\n",
    "\n",
    "        sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
