{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Streaming application using Spark Structured Streaming\n",
    "\n",
    "Implement Spark Structured Streaming to consume the data from the producer and perform predictive analytics.\n",
    "\n",
    "### 2.1 Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "import os\n",
    "from pyspark.sql.types import StructType, IntegerType, TimestampType,StringType,DateType, ArrayType, StructField, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import year, month, dayofmonth, weekofyear, dayofweek\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import window, col\n",
    "from pyspark.sql.functions import to_json, struct\n",
    "from pyspark.sql.functions import collect_list\n",
    "from pyspark.sql.functions import unix_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create SparkSession\n",
    "\n",
    "Get SparkContext object from SparkSession. Using SparkConf object to configure the Spark app with the setting as follow: (Sangat, P., 2021)\n",
    "\n",
    "- Application name: Pedestrain Traffic Predictive Analysis\n",
    "- Session timezone: UTC\n",
    "- run with 2 local cores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "# local[2]: run Spark in local mode with2 working processors as logical cores on your machine\n",
    "master = \"local[2]\"\n",
    "\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Pedestrain Traffic Predictive Analysis\"\n",
    "\n",
    "#### set melbourne timezone\n",
    "\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# Using SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", 'UTC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define the data schema (for sensor location CSV)\n",
    "Define the data schema for the sensor location CSV file following the data types in the metadata file except for the location columns (Use StringType for “location” column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_B = StructType() \\\n",
    "      .add(\"sensor_id\",IntegerType(),True) \\\n",
    "      .add(\"sensor_description\",StringType(),True) \\\n",
    "      .add(\"sensor_name\",StringType(),True) \\\n",
    "      .add(\"installation_date\",DateType(),True) \\\n",
    "      .add(\"status\",StringType(),True) \\\n",
    "      .add(\"note\",StringType(),True) \\\n",
    "      .add(\"direction_1\",StringType(),True) \\\n",
    "      .add(\"direction_2\",StringType(),True) \\\n",
    "      .add(\"latitude\",FloatType(),True)\\\n",
    "      .add(\"longitude\",FloatType(),True)\\\n",
    "      .add(\"location\",StringType(),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_senloc = spark.read.options(header = 'True', delimiter=',').schema(schema_B) \\\n",
    "  .csv(\"Pedestrian_Counting_System_-_Sensor_Locations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Read Stream from the Kafka Topic\n",
    "\n",
    "Connection to Kafka Producer/Broker and subscribe to the topic - pedestrain_count and load data from Kafka topic with <code>readStream</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic = \"pedestrain_count\"\n",
    "df = spark \\\n",
    "      .readStream \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "      .option(\"subscribe\", topic) \\\n",
    "      .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the key/value from the kafka data stream to string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Persist the data in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write into parquet files the unsuccessful requests partitioned by status code\n",
    "query_file_sink = df.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"parquet/pedcount_df\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet/pedcount_df/checkpoint\")\\\n",
    "        .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop the file_sink query\n",
    "query_file_sink.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the schema for the structured datastream received\n",
    "schema1 = ArrayType(StructType([    \n",
    "    StructField('ID', IntegerType(), True), \n",
    "    StructField('Date_Time', StringType(), True), \n",
    "    StructField('Year', IntegerType(), True),\n",
    "    StructField('Month', StringType(), True),\n",
    "    StructField('Mdate', IntegerType(), True),\n",
    "    StructField('Day', StringType(), True),\n",
    "    StructField('Time', IntegerType(), True),\n",
    "    StructField('Sensor_ID', IntegerType(), True),\n",
    "    StructField('Sensor_Name', StringType(), True),\n",
    "    StructField('Hourly_Counts', IntegerType(), True), \n",
    "    StructField('date', StringType(), True), \n",
    "    StructField('time1', StringType(), True),\n",
    "    StructField('am/pm', StringType(), True)\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `explode` function to flatten the nested columns, then proceed with to rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pedcount = df.select(F.from_json(F.col(\"value\").cast(\"string\"), schema1).alias('parsed_value'))\n",
    "df_pedcount = df_pedcount.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value')) \n",
    "\n",
    "# rename the columns\n",
    "df_formatted = df_pedcount.select(\n",
    "                    F.col(\"unnested_value.ID\").alias(\"ID\"),\n",
    "                    F.col(\"unnested_value.Date_Time\").alias(\"Date_Time\"),\n",
    "                    F.col(\"unnested_value.Year\").alias(\"Year\"),\n",
    "                    F.col(\"unnested_value.Month\").alias(\"Month\"),\n",
    "                    F.col(\"unnested_value.Mdate\").alias(\"Mdate\"),\n",
    "                    F.col(\"unnested_value.Day\").alias(\"Day\"),\n",
    "                    F.col(\"unnested_value.Time\").alias(\"Time\"),\n",
    "                    F.col(\"unnested_value.Sensor_ID\").alias(\"Sensor_ID\"),\n",
    "                    F.col(\"unnested_value.Sensor_Name\").alias(\"Sensor_Name\"),\n",
    "                    F.col(\"unnested_value.Hourly_Counts\").alias(\"Hourly_Counts\"),\n",
    "                    F.col(\"unnested_value.date\").alias(\"date\"),\n",
    "                    F.col(\"unnested_value.time1\").alias(\"time1\"),\n",
    "                    F.col(\"unnested_value.am/pm\").alias(\"am/pm\")      \n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Add required columns for the model prediction\n",
    "\n",
    "Perform the following transformations to prepare the columns for model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "df_formatted = df_formatted.withColumn('Date_Time', to_timestamp(df_formatted.Date_Time, 'MM/dd/yyyy hh:mm:ss a'))\n",
    "df_formatted = df_formatted.withColumn('date', to_timestamp(df_formatted.date, 'MM/dd/yyyy'))\n",
    "\n",
    "# Create a date format column named “next_date”\n",
    "df_nextdate = df_formatted.withColumn('next_date',F.date_add(df_formatted['Date_Time'], 1))\n",
    "\n",
    "# Create the column named “next_Mdate”\n",
    "df_mdate = df_nextdate.withColumn('next_Mdate',dayofmonth(\"next_date\"))\n",
    "\n",
    "# Create the column named “next_day_week”\n",
    "df_nextdayweek = df_mdate.withColumn('next_day_week',weekofyear(\"next_date\"))\n",
    "\n",
    "# Create the column named “next_day_of_week”\n",
    "df_nextdayofweek = df_nextdayweek.withColumn('next_day_of_weekk',dayofweek(\"next_date\")-1)\n",
    "new_pedcount = df_nextdayofweek.withColumn(\"next_day_of_week\", \\\n",
    "              when(df_nextdayofweek[\"next_day_of_weekk\"] == 0, 7)\\\n",
    "                        .otherwise(df_nextdayofweek[\"next_day_of_weekk\"]))\n",
    "\n",
    "# Rename the column “Hourly_Count” as “prev_count\n",
    "new_pedcount = new_pedcount.withColumnRenamed('Hourly_Counts', 'prev_count')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "# query = new_pedcount \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .trigger(processingTime='5 seconds') \\\n",
    "#     .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Load prediction model\n",
    "\n",
    "Load the given prediction model to predict the next day's pedestrain count between 9am - 11.59pm.The provided model “count_estimation_pipeline_model” is a simplified version to predict the hourly count given the input of sensor ID, week of the year, day of the month, day of the week, time, and previous day’s hourly count at the same hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the Pipeline Model From the filesystem\n",
    "from pyspark.ml import PipelineModel\n",
    "pipelineModel = PipelineModel.load('count_estimation_pipeline_model')\n",
    "\n",
    "print(pipelineModel.stages[-1]._java_obj.paramMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out times not between **9am to 11:59pm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pedcount = new_pedcount.filter(F.col('Time') >= 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pipelineModel.transform(pred_pedcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persist the prediction result in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write into parquet files the unsuccessful requests partitioned by status code\n",
    "query_file_prediction = predictions_df.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"parquet/prediction_df\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet/prediction_df/checkpoint1\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop the file_sink query\n",
    "query_file_prediction.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Data processing\n",
    "Process the data with the prediction result, following the below requirements:\n",
    "\n",
    "    a. Get number of hours that the predicted pedestrain count would exceeded 2000 on each day for each sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add threshold column\n",
    "predictions_df = predictions_df.withColumn('above_threshold', F.when(F.col('prediction') < 2001, 0).otherwise(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set watermark for 1 day and window that slide 1 day and get the \n",
    "# sum of hours that pedcount exceeded 2000\n",
    "windowedCounts = predictions_df \\\n",
    "    .withWatermark(\"date\", \"1 Day\") \\\n",
    "    .groupBy(window(predictions_df.date, \"1 Day\", \"1 Day\"),predictions_df.Sensor_ID )\\\n",
    "    .agg(F.sum(\"above_threshold\").alias(\"no_of_hours\"))\\\n",
    "    .select(\"window\",\"Sensor_ID\",\"no_of_hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to show values received from input dataframe\n",
    "def foreach_batch_function(df, epoch_id):\n",
    "    df.show(20, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the result inside the notebook\n",
    "query = windowedCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .foreachBatch(foreach_batch_function) \\\n",
    "    .queryName(\"hour_count\") \\\n",
    "    .option(\"truncate\",\"false\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    b. Conbine the predicted results that exceeded 2000 with sensor longitude and latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column “sendor_id” as 'sen_id'\n",
    "df_senloc = df_senloc.withColumnRenamed('sensor_id', 'sen_id')\n",
    "\n",
    "# Select onyl the required columns\n",
    "df_senloc = df_senloc.select(\"sen_id\", \"latitude\", \"longitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining the streaming dataFrame - predictions_df with a static dataFrame - df_senloc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Join Dataframes\n",
    "ped_location = predictions_df.join(df_senloc,predictions_df.Sensor_ID==df_senloc.sen_id,how='inner')\n",
    "\n",
    "# filter the df and select the required columns\n",
    "ped_2000 = ped_location.filter(col(\"above_threshold\") == 1)\\\n",
    "                .select('Date_Time', 'next_date', 'next_Mdate', 'next_day_week', 'next_day_of_week', 'prediction', 'Sensor_ID', 'latitude', 'longitude', 'prediction')\n",
    "\n",
    "# construct key and value columns\n",
    "new_ped = ped_2000.select(col(\"next_date\"), to_json(struct(\"*\")), col('Date_Time')).toDF(\"key\", \"value\", \"datee\")\n",
    "\n",
    "# aggregate the value with the same day (key) with watermakr of 1 day\n",
    "windowedCounts1 = new_ped \\\n",
    "    .withWatermark(\"datee\", \"1 Day\") \\\n",
    "    .groupBy(window(\"datee\", \"1 Day\"),\"key\" )\\\n",
    "    .agg(collect_list('value').alias(\"value\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrte the stream back to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_ped1 = windowedCounts1.selectExpr(\"CAST(key AS STRING) AS key\", \"CAST(value AS STRING) AS value\")\n",
    "topic1 = 'ped_count_2000'\n",
    "query= new_ped1.writeStream \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "            .option('topic', topic1) \\\n",
    "            .option(\"checkpointLocation\", \"kafka/ped_count_2000/checkpoint2\")\\\n",
    "            .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "# query = windowedCounts1 \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .trigger(processingTime='5 seconds') \\\n",
    "#     .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
